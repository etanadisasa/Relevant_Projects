{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/hdp/3.1.0.0-78/spark2')\n",
    "# Installed findspark first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "# Installed pyspark prior to this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here instead of creating the process distribution acoss both vms using 'sc = pyspark.SparkContext\n",
    "# (appName=\"InvertedIndex\")' I decided to only use the local machine\n",
    "sc = pyspark.SparkContext(\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "# Use the SparkSql module to read data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('InvertedIndex').getOrCreate()\n",
    "# Here is what runs out process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('hdfs:///user/vagrant/DataForProject.csv', mode=\"DROPMALFORMED\", inferSchema=True, header=True)\n",
    "# Here, I called the CSV file from the HDFS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'CreationDate', 'Tags', 'PostTypeId']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "# The four columns for this dataframe are below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('CreationDate', 'timestamp'),\n",
       " ('Tags', 'string'),\n",
       " ('PostTypeId', 'int')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n",
    "# The datatypes of each column is listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+----------+\n",
      "|      id|       CreationDate|                Tags|PostTypeId|\n",
      "+--------+-------------------+--------------------+----------+\n",
      "|45673148|2017-08-14 11:07:44|<scala><playframe...|         1|\n",
      "|45673149|2017-08-14 11:07:45|<ios><iphone><cor...|         1|\n",
      "|45673150|2017-08-14 11:07:45|<apache-kafka><ap...|         1|\n",
      "|45673151|2017-08-14 11:07:47|                null|         2|\n",
      "|45673152|2017-08-14 11:08:06|                null|         2|\n",
      "|45673153|2017-08-14 11:08:09|<node.js><google-...|         1|\n",
      "|45673154|2017-08-14 11:08:13|                null|         2|\n",
      "|45673155|2017-08-14 11:08:28|                null|         2|\n",
      "|45673157|2017-08-14 11:08:29|        <regex><sed>|         1|\n",
      "|45673158|2017-08-14 11:08:30|                null|         2|\n",
      "+--------+-------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['<scala>', '<playframework>', '<sbt>'], 45673148],\n",
       " [['<ios>', '<iphone>', '<cordova>', '<itunesconnect>', '<ipa>'], 45673149],\n",
       " [['<apache-kafka>', '<apache-kafka-streams>'], 45673150],\n",
       " [None, 45673151],\n",
       " [None, 45673152],\n",
       " [['<node.js>', '<google-authentication>'], 45673153],\n",
       " [None, 45673154],\n",
       " [None, 45673155],\n",
       " [['<regex>', '<sed>'], 45673157],\n",
       " [None, 45673158]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below, I used the regexp_replace function to call the Tags column and insert a ',' \n",
    "## between each word that is tagged by < > inorder to separate them. \n",
    "newsdf = df.withColumn('Tags', regexp_replace(col('Tags'), \"\\\\>\\\\<\", \">,<\"))\n",
    "## After the this process, the 'split' function split each word into a list separated by ','\n",
    "TagsId = newsdf.withColumn(\"Tags\", split(\"Tags\", \",\"))\n",
    "## The following line selects the two columsn (Tags and id); and names the modified dataframe as 'TagsId'\n",
    "TagsId = TagsId.select(\"Tags\",\"id\")\n",
    "## For displaying a clean, I converted the dataframe into RDD (TagsIdDisplay) and kept the original (TagsId). \n",
    "TagsIdDisplay = TagsId.rdd.map(list)\n",
    "TagsIdDisplay.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<scala>', 45673148],\n",
       " ['<playframework>', 45673148],\n",
       " ['<sbt>', 45673148],\n",
       " ['<ios>', 45673149],\n",
       " ['<iphone>', 45673149],\n",
       " ['<cordova>', 45673149],\n",
       " ['<itunesconnect>', 45673149],\n",
       " ['<ipa>', 45673149],\n",
       " ['<apache-kafka>', 45673150],\n",
       " ['<apache-kafka-streams>', 45673150]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nevertheless, we can simply make a key/value connection between the tags and ids by using the 'explode function'\n",
    "# Here the data fame is called again to take each tag separated by ',' in 'Tags' column and map it the prospectie\n",
    "# id in the 'id' column\n",
    "detailed = newsdf.withColumn(\"Tags\", explode(split(\"Tags\", \"\\\\,\")))\n",
    "detailed = detailed.select(\"Tags\",\"id\")\n",
    "detailedDisplay = detailed.rdd.map(list)\n",
    "detailedDisplay.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<.net-core-1.1>', [45712067]],\n",
       " ['<8051>', [46722156]],\n",
       " ['<activemerchant>', [46720159]],\n",
       " ['<aframe>', [46713776, 46797635, 46799726, 46838749, 46860404]],\n",
       " ['<aliases>', [46792950]],\n",
       " ['<android-fusedlocation>', [46803119]],\n",
       " ['<application-structure>', [46796373]],\n",
       " ['<arm-template>', [46833296]],\n",
       " ['<aws-codepipeline>', [46720870]],\n",
       " ['<azure-iot-hub>', [45703990, 45716714, 46838666]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once we complete the mapping, we now group each observation in the 'Tags' column its consecutive id. \n",
    "indexed = detailed.groupby(\"Tags\").agg(collect_list(\"id\"))\n",
    "# The groupby function may also be used if we use the 'id' keys and 'Tags' as values. \n",
    "indexedDisplay = indexed.rdd.map(list)\n",
    "# I am converting the Dataframe to RDD just for the sake of displaying the outcome. \n",
    "indexedDisplay.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas\n",
    "# Below is the code I used to export the result into a json file and save in the Hadoop file system. .\n",
    "import json\n",
    "collected_df = str(indexedDisplay.collect())\n",
    "with open('FinalProject.json', 'w') as outfile:\n",
    "    json.dump(collected_df, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
